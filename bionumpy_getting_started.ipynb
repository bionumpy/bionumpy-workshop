{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with BioNumPy\n",
    "\n",
    "This task will get you started with BioNumPy. We will cover two important concepts:\n",
    "\n",
    "1) Using numpy on BioNumPy datasets\n",
    "2) Reading datasets and performing operations on dataset chunks\n",
    "\n",
    "\n",
    "## Install and import\n",
    "\n",
    "BioNumPy can easily be installed through pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bionumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the installation worked by importing BioNumPy and encode a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bionumpy as bnp\n",
    "sequence = bnp.as_encoded_array(\"ACTG\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Working with BioNumPy datasets\n",
    "\n",
    "\n",
    "BioNumPy datasets are usually created by reading files (e.g. fastq, vcfs etc), but we can also create small datasets using the\n",
    "`bnp.as_encoded_array()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTGACG\n",
      "ACA\n",
      "ACACGGAAC\n"
     ]
    }
   ],
   "source": [
    "sequences = bnp.as_encoded_array([\n",
    "    \"ACTGACG\",\n",
    "    \"ACA\",\n",
    "    \"ACACGGAAC\"\n",
    "])\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequences` object is encoded and is represented using an efficient NumPy-like data structure, but we don't need to know the internal details, and can use it just like any NumPy-matrix, with the additional benefit of thinking of the data as DNA and not numbers. For instance, finding the position of Gs is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True False False  True]\n",
      "[False False False]\n",
      "[False False False False  True  True False False False]\n"
     ]
    }
   ],
   "source": [
    "is_g = sequences == \"G\"\n",
    "print(is_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then e.g. take the sum of this mask to count the number of Gs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(is_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think happens if you specify `axis=1` on np.sum()? What does the output tell you? Try running the code below and see if you can make sense of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(is_g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue, check that you have understood how NumPy can be used on BioNumPy data:\n",
    "\n",
    "1) Make a mask with the posisions of C\n",
    "2) Count how many bases are either C or G\n",
    "3) Compute the GC-content (ps: You can get use of `np.mean` here\n",
    "4) Make a new set of sequences where the first base pairs are removed\n",
    "\n",
    "Solution (only look after you have tried your self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_c = sequences == \"C\"\n",
    "is_c_or_g = is_c | is_g\n",
    "# number of c or g:\n",
    "np.sum(is_c_or_g)\n",
    "\n",
    "print(\"GC content:\")\n",
    "gc_content = np.mean(is_c_or_g)\n",
    "print(gc_content)\n",
    "\n",
    "print(\"Stripped sequences:\")\n",
    "stripped_sequences = sequences[:, 1:]\n",
    "print(stripped_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with files\n",
    "\n",
    "In the previous task, we worked with a very small data set of only three sequences. When working with large datasets, we want to avoid reading the whole data set into memory. Instead, BioNumPy reads chunks of data, and we typically analyse each chunk seperately and combine the results in the end.\n",
    "\n",
    "In this and the coming exercises, we will work with ChIP-seq data. We start by downloading FASTQ reads for a CTCF ChIP-seq experiment from the Encode Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.encodeproject.org/files/ENCFF000RWH/@@download/ENCFF000RWH.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, you will get a file `ENCFF000RWH.fastq`. You can open and read a chunk from the file with BioNumPy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "chunk = f.read_chunk()\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chunk` is now an object containing a part of the file. We can access the sequences, names and qualities of the entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk.sequence)\n",
    "print(chunk.name)\n",
    "print(chunk.quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects work similarily to the `sequences` object in the previous task. Note that we can index the chunk. For instance, getting the first three reads can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Try to the average base quality value of all the reads in the chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_base_quality = # ... implement your code here\n",
    "print(average_base_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    "* Find the average base quality for each read in this chunk (hint: axis=1)\n",
    "* How many reads have average base quality lower than 20?\n",
    "* Subset the chunk so that you are left with reads with average base qualities >= 20.\n",
    "* How many reads are there in your new filtered chunk?\n",
    "* Put your code for filtering in a function. The function should take a chunk as an argument and return a new \"filtered\" chunk.\n",
    "\n",
    "\n",
    "Solution below (don't see before you've tried yourself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reads(chunk):\n",
    "    mask = np.mean(chunk.quality, axis=1) >= 20\n",
    "    return chunk[mask]\n",
    "\n",
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "chunk = f.read_chunk()\n",
    "print(chunk)\n",
    "\n",
    "filtered_chunk = filter_reads(chunk)\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with chunks from files\n",
    "Above, we've written code for filtering a chunk of sequences. We can then read chunks iteratively from a file, and run our function on each chunk. This way, we keep memory usage low, while working on large-enough chunks to get significant speedup from NumPy (instead of working on single fasta entries, which is common when writing Vanilla Python programs).\n",
    "\n",
    "Below, we read chunks iteratively using the `file.read_chunks()` method, filter each chunk and write the resulting chunks to a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "out_file = bnp.open(\"ENCFF000RWH_filtered.fastq\", \"w\")\n",
    "\n",
    "for chunk in f.read_chunks():\n",
    "    print(chunk)\n",
    "    filtered_chunk = filter_reads(chunk)\n",
    "    print(filtered_chunk)\n",
    "    out_file.write(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Combining analysis results from chunks\n",
    "\n",
    "A common pattern when working with big datasets is to perform an analysis on parts of the dataset (chunks) and combine the results.\n",
    "\n",
    "For instance, assume we want to compute the average base quality for the whole data set, but we don't want to read the whole data set into memory.\n",
    "\n",
    "BioNumPy lets you do computation on single chunks, and provides utility functions for merging the results. This is done by adding the `bnp.streamable()` decorator.\n",
    "\n",
    "For instance, here we have defined a function that computes the number of matches of the subsequence CCCTC in **a single chunk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionumpy.sequence.string_matcher import match_string\n",
    "def count_reads_with_matches(chunk):\n",
    "    matches = match_string(chunk.sequence, \"CCCTC\")\n",
    "    return np.sum(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then want the number of matches for our whole read, we could call the function per chunk like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH_filtered.fastq\")\n",
    "results = []\n",
    "for chunk in f.read_chunks():\n",
    "    results.append(count_reads_with_matches(chunk))\n",
    "\n",
    "print(sum(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid doing the for-loop above, and get some clean code, BioNumPy provides a decorator `@streamable`\n",
    "that can be added above a function in order to make the function able to handle multiple chunks. BioNumPy will automatically\n",
    "run the function on each chunk and combine the results using the function provided with the decorator, in this case `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bnp.streamable(sum)\n",
    "def count_reads_with_matches(chunk):\n",
    "    matches = match_string(chunk.sequence, \"CCCTC\")\n",
    "    return np.sum(matches)\n",
    "\n",
    "chunks = bnp.open(\"ENCFF000RWH_filtered.fastq\").read_chunks()\n",
    "print(count_reads_with_matches(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 5\n",
    "Using builtin BioNumPy-functions on chunks.\n",
    "\n",
    "BioNumPy also provides some useful utility-functions for combining results from multiple chunks. One such function is\n",
    "`bnp.mean` which can take a generator of cunks and work on the generator as if it only got one large chunk.\n",
    "\n",
    "For instance, assume we write a function that finds all matches within a chunk:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@bnp.streamable\n",
    "def get_matches(chunk, sequence):\n",
    "    return match_string(chunk.sequence, sequence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calling this function on chunks gives us a generator containing the matches for each chunk:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH_filtered.fastq\")\n",
    "chunks = f.read_chunks()\n",
    "matches = get_matches(chunks, \"CCCTC\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we call `bnp.mean` on matches, `bnp.mean` will compute the mean of all the matches masks as if it only got one single mask for the whole data set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
