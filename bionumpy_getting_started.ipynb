{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with BioNumPy\n",
    "\n",
    "This task will get you started with BioNumPy. We will cover three important concepts:\n",
    "\n",
    "1) Using NumPy on BioNumPy datasets\n",
    "2) Reading datasets and performing operations on dataset chunks\n",
    "3) Combining analysis results from chunks of data\n",
    "\n",
    "\n",
    "## Install and import\n",
    "\n",
    "BioNumPy can easily be installed through pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bionumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the installation worked by importing BioNumPy and encode a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bionumpy as bnp\n",
    "sequence = bnp.as_encoded_array(\"ACTG\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to BioNumPy datasets\n",
    "\n",
    "BioNumPy datasets can consist of things like DNA sequences, sequence names, base qualities, proteins sequences, etc.\n",
    "They are usually created by reading files (e.g. fastq, vcfs etc), but we can also create small datasets using the\n",
    "`bnp.as_encoded_array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = bnp.as_encoded_array([\n",
    "    \"ACTGACG\",\n",
    "    \"ACA\",\n",
    "    \"ACACGGAAC\"\n",
    "])\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequences` object is encoded and represented using an efficient NumPy-like data structure. However, BioNumPy doesn't require you to know about any of internals or details, and we can just treat the data as NumPy-matrix consisting of DNA and not numbers. For instance, getting a boolean mask with the positions of Gs is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_g = sequences == \"G\"\n",
    "print(is_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the `np.sum` of this mask to count the number of Gs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(is_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "What do you think happens if you specify `axis=1` on np.sum()? What does the output tell you? Try running the code below and see if you can make sense of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(is_g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Before you continue, check that you have understood how NumPy can be used on BioNumPy data:\n",
    "\n",
    "1) Make a mask with the positions of C\n",
    "2) Count how many bases are either C or G\n",
    "3) Compute the GC-content (PS: You might get use of `np.mean`)\n",
    "4) Make a new set of sequences where the first base pairs are removed\n",
    "\n",
    "Solution (only look after you have tried your self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_c = sequences == \"C\"\n",
    "is_c_or_g = is_c | is_g  # the | operator is \"or\"\n",
    "# number of c or g:\n",
    "np.sum(is_c_or_g)\n",
    "\n",
    "print(\"GC content:\")\n",
    "gc_content = np.mean(is_c_or_g)  # alternatively sum and divide by number of bases\n",
    "print(gc_content)\n",
    "\n",
    "print(\"Stripped sequences:\")\n",
    "stripped_sequences = sequences[:, 1:]  # we index on last dimension (columns)\n",
    "print(stripped_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with files\n",
    "\n",
    "In the previous task, we worked with a very small dataset consisting of only three sequences. When working with larger datasets, we want to avoid reading the whole data into memory. Instead, BioNumPy reads chunks of data, and we typically analyse each chunk seperately and combine the results in the end if necessary.\n",
    "\n",
    "In this and the coming exercises, we will work with ChIP-seq data. We start by downloading FASTQ reads for a CTCF ChIP-seq experiment from the Encode Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.encodeproject.org/files/ENCFF000RWH/@@download/ENCFF000RWH.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, you will get a file `ENCFF000RWH.fastq.gz`. You can open and read a chunk from the file with BioNumPy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "chunk = f.read_chunk()\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chunk` is now an object containing a part of the file. BioNumPy automatically detected that this is a FASTQ file, and chooses a suitable data structure. We can access the sequences, names and qualities from this data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk.sequence)\n",
    "print(chunk.name)\n",
    "print(chunk.quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects work similarily to the `sequences` object in the previous task, meaning that NumPy-functions are compatible with them. We can also index the chunk like a NumPy array. For instance, getting the first three reads can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and similarily, getting the first three bases of each read is as simple as:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(chunk[:, 0:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "\n",
    "* Compute the average base quality value of all the reads in the chunk\n",
    "* Compute the average base quality without considering the first 5 and last 5 base pairs of all the reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that NumPy-functions such as `np.mean` and `np.sum` can take an axis-argument. `axis=0` performs the operation over the first axis (e.g. computes one number for every base if you have reads) wile `axis=1` performs the operation over the rows (e.g. computes one number for every read).\n",
    "\n",
    "**Task**\n",
    "\n",
    "* Find the average base quality for **each read** in this chunk (hint: axis=1)\n",
    "* How many reads have average base quality lower than 20?\n",
    "* Subset the chunk so that you are left with reads with average base qualities >= 20.\n",
    "* How many reads are there in your new filtered chunk?\n",
    "* Put your code for filtering the reads in a function. The function should take a chunk as an argument and return a new \"filtered\" chunk.\n",
    "\n",
    "\n",
    "Solution below (don't look before you've tried yourself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reads(chunk):\n",
    "    mask = np.mean(chunk.quality, axis=1) >= 20\n",
    "    return chunk[mask]\n",
    "\n",
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "chunk = f.read_chunk()\n",
    "print(chunk)\n",
    "\n",
    "filtered_chunk = filter_reads(chunk)\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with chunks from files\n",
    "Above, we've written code for filtering a chunk of sequences. This allows us to read chunks iteratively from a file, and run our function on each chunk. Working on chunks like this let's us keep memory usage low while still getting significant speedup from NumPy (as compared to working on single reads, which is common when writing vanilla Python programs).\n",
    "\n",
    "Below, we read chunks iteratively using the `file.read_chunks()` method, filter each chunk and write the resulting filtered chunks to a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH.fastq.gz\")\n",
    "out_file = bnp.open(\"ENCFF000RWH_filtered.fastq\", \"w\")\n",
    "\n",
    "for chunk in f.read_chunks():\n",
    "    print(chunk)\n",
    "    filtered_chunk = filter_reads(chunk)\n",
    "    print(filtered_chunk)\n",
    "    out_file.write(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the code above runs before continuing. You may need to change the call to `filter_reads` to match the name of the function you created for filtering.\n",
    "\n",
    "## Part 4: Combining analysis results from chunks\n",
    "\n",
    "A common pattern when working with big datasets is to perform an analysis on parts of the dataset (chunks) and combine the results.\n",
    "\n",
    "For instance, assume we want to compute the average base quality for the whole data set, but we don't want to read the whole data set into memory.\n",
    "\n",
    "BioNumPy lets you do computation on single chunks, and provides utility functions for merging the results. This is done by adding the `bnp.streamable()` decorator.\n",
    "\n",
    "For instance, here we have defined a function that computes the number of matches of the subsequence CCCTC in **a single chunk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionumpy.sequence.string_matcher import match_string\n",
    "def count_reads_with_matches(chunk):\n",
    "    # Makes a boolean mask with True where we have a match and False elsewhere\n",
    "    matches = match_string(chunk.sequence, \"CCCTC\")\n",
    "    return np.sum(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then want the number of matches for our whole read dataset, we could call the function per chunk like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH_filtered.fastq\")\n",
    "results = []\n",
    "for chunk in f.read_chunks():\n",
    "    results.append(count_reads_with_matches(chunk))\n",
    "\n",
    "print(sum(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid doing the for-loop above, BioNumPy provides a decorator `@streamable()`\n",
    "that can be added above a function in order to make the function able to handle multiple chunks. BioNumPy will automatically\n",
    "run the function on each chunk and combine the results using the function provided with the decorator, in this case `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bnp.streamable(sum)\n",
    "def count_reads_with_matches(chunk):\n",
    "    matches = match_string(chunk.sequence, \"CCCTC\")\n",
    "    return np.sum(matches)\n",
    "\n",
    "chunks = bnp.open(\"ENCFF000RWH_filtered.fastq\").read_chunks()\n",
    "print(count_reads_with_matches(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 5: Using builtin BioNumPy-functions on chunks.\n",
    "\n",
    "BioNumPy also provides some useful utility-functions for combining results from multiple chunks. One such function is\n",
    "`bnp.mean` which can take a generator of results computed on chunks and work on the generator as if it only got one big dataset.\n",
    "\n",
    "For instance, assume we write a function that finds all matches within a chunk:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@bnp.streamable()\n",
    "def get_matches(chunk, sequence):\n",
    "    return match_string(chunk.sequence, sequence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calling this function on multiple chunks gives us a generator containing the matches for each chunk:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = bnp.open(\"ENCFF000RWH_filtered.fastq\")\n",
    "chunks = f.read_chunks()\n",
    "matches = get_matches(chunks, \"CCCTC\")\n",
    "print(type(matches))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matches is a stream (generator) that will yield the mask for each chunk if we iterate over it.\n",
    "If we call `bnp.mean` on matches, `bnp.mean` will compute the mean of all the masks in `matches`, combine them correctly,\n",
    "and return one single number as if it only got **one single mask** for the whole data set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_matches_per_base = bnp.mean(matches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this pattern may seem a bit cumbersome until you get used to it, but it enables analysis on larger datasets that can fit into memory.\n",
    "A common usecase of this pattern is when one wants to compute an average or other statistic over a big data set.\n",
    "\n",
    "**TASK** Try making a function that simply returns the base qualities for a chunk, and use `bnp.mean` to get the average base qualities for the whole data set.\n",
    "\n",
    "Compute the average base qualities of the filtered and unfiltered fastq reads (`ENCFF000RWH_filtered.fastq` and `ENCFF000RWH.fastq.gz`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Is the average base quality higher in the filtered reads?\n",
    "\n",
    "Solution (don't look before you've tried solving it):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@bnp.streamable()\n",
    "def get_base_qualities(chunk):\n",
    "    return chunk.quality\n",
    "\n",
    "f = bnp.open(\"ENCFF000RWH_filtered.fastq\")  # try with filtered and unfiltered\n",
    "chunks = f.read_chunks()\n",
    "qualities = get_base_qualities(chunks)\n",
    "print(bnp.mean(qualities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Final notes\n",
    "\n",
    "If you've successfully done all the exerices in this document, you are ready to use BioNumPy on a wide range of data sets. A typical workflow with BioNumPy typically looks like this:\n",
    "\n",
    "1) Read one or more data sets with `bnp.open`\n",
    "2) Use `np`-functions to slice, index or to analyse the data\n",
    "3) Either write functions that you decorate with `@streamable()` or use builtin-functions to combine results from multiple chunks\n",
    "\n",
    "In the coming tasks, you will also see that BioNumPy has a lot of builtin utility-function for typical analysis (such as motif matching, kmers, etc)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
